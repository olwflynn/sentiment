{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "print(t.fit_on_texts(docs))\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(t.fit_on_texts(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /Users/OFlynn/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: requests in /Users/OFlynn/miniconda3/lib/python3.6/site-packages (from plotly)\n",
      "Requirement already satisfied: decorator>=4.0.6 in /Users/OFlynn/miniconda3/lib/python3.6/site-packages (from plotly)\n",
      "Requirement already satisfied: pytz in /Users/OFlynn/miniconda3/lib/python3.6/site-packages (from plotly)\n",
      "Requirement already satisfied: six in /Users/OFlynn/miniconda3/lib/python3.6/site-packages (from plotly)\n",
      "Requirement already satisfied: nbformat>=4.2 in /Users/OFlynn/miniconda3/lib/python3.6/site-packages (from plotly)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/OFlynn/miniconda3/lib/python3.6/site-packages (from requests->plotly)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/OFlynn/miniconda3/lib/python3.6/site-packages (from requests->plotly)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/OFlynn/miniconda3/lib/python3.6/site-packages (from requests->plotly)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/OFlynn/miniconda3/lib/python3.6/site-packages (from requests->plotly)\n",
      "Requirement already satisfied: jupyter-core in /Users/OFlynn/miniconda3/lib/python3.6/site-packages (from nbformat>=4.2->plotly)\n",
      "Requirement already satisfied: traitlets>=4.1 in /Users/OFlynn/miniconda3/lib/python3.6/site-packages (from nbformat>=4.2->plotly)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Users/OFlynn/miniconda3/lib/python3.6/site-packages (from nbformat>=4.2->plotly)\n",
      "Requirement already satisfied: ipython-genutils in /Users/OFlynn/miniconda3/lib/python3.6/site-packages (from nbformat>=4.2->plotly)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting nltk\n",
      "  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 1.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/OFlynn/miniconda3/lib/python3.6/site-packages (from nltk)\n",
      "Building wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/OFlynn/Library/Caches/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.3\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "## Plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "# Others\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id  cool       date  funny               review_id  \\\n",
      "0  0W4lkclzZThpx3V65bVgig     0 2016-05-28      0  v0i_UHJMo_hPBq9bxWvW4w   \n",
      "1  AEx2SYEUJmTxVVB18LlCwA     0 2016-05-28      0  vkVSCC7xljjrAI4UGfnKEQ   \n",
      "2  VR6GpWIda3SfvPC-lg9H3w     0 2016-05-28      0  n6QzIUObkYshz4dz2QRJTw   \n",
      "3  CKC0-MOWMqoeWf6s-szl8g     0 2016-05-28      0  MV3CcKScW05u5LVfF6ok0g   \n",
      "4  ACFtxLv8pGrrxMm6EgjreA     0 2016-05-28      0  IXvOzsEMYtiJI0CARmj77Q   \n",
      "\n",
      "   stars                                               text  useful  \\\n",
      "0      5  Love the staff, love the meat, love the place....       0   \n",
      "1      5  Super simple place but amazing nonetheless. It...       0   \n",
      "2      5  Small unassuming place that changes their menu...       0   \n",
      "3      5  Lester's is located in a beautiful neighborhoo...       0   \n",
      "4      4  Love coming here. Yes the place always needs t...       0   \n",
      "\n",
      "                  user_id  \n",
      "0  bv2nCi5Qv5vroFiqKGopiw  \n",
      "1  bv2nCi5Qv5vroFiqKGopiw  \n",
      "2  bv2nCi5Qv5vroFiqKGopiw  \n",
      "3  bv2nCi5Qv5vroFiqKGopiw  \n",
      "4  bv2nCi5Qv5vroFiqKGopiw  \n"
     ]
    }
   ],
   "source": [
    "# read the entire file into a python array\n",
    "with open('review.json', 'r') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "# remove the trailing \"\\n\" from each line\n",
    "data = map(lambda x: x.rstrip(), data)\n",
    "\n",
    "data_json_str = \"[\" + ','.join(data) + \"]\"\n",
    "\n",
    "# now, load it into pandas\n",
    "data_df = pd.read_json(data_json_str)\n",
    "print(data_df.head())\n",
    "# df= df.dropna()\n",
    "# df = df[df.stars.apply(lambda x: x.isnumeric())]\n",
    "# df = df[df.stars.apply(lambda x: x !=\"\")]\n",
    "# df = df[df.text.apply(lambda x: x !=\"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_short = data_df[:10000]\n",
    "data_df_short.to_json('reviews_short.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= data_df_short.dropna()\n",
    "# df = data_df_short[data_df_short.stars.apply(lambda x: x.isdigit())]\n",
    "df = data_df_short[data_df_short.stars.apply(lambda x: x !=\"\")]\n",
    "df = data_df_short[data_df_short.text.apply(lambda x: x !=\"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['stars'].map(lambda x : 1 if int(x) > 3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/OFlynn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Text Normalizing function. Part of the following function was taken from this link. \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk, re\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    ## Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    ## Stemming\n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    return text\n",
    "## apply the above function to df['text']\n",
    "df['text'] = df['text'].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create sequence\n",
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "data = pad_sequences(sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 4000 samples\n",
      "Epoch 1/3\n",
      "6000/6000 [==============================] - 15s 3ms/step - loss: 0.5621 - acc: 0.7113 - val_loss: 0.4726 - val_acc: 0.7915\n",
      "Epoch 2/3\n",
      "6000/6000 [==============================] - 14s 2ms/step - loss: 0.3479 - acc: 0.8563 - val_loss: 0.4339 - val_acc: 0.8063\n",
      "Epoch 3/3\n",
      "6000/6000 [==============================] - 14s 2ms/step - loss: 0.2287 - acc: 0.9137 - val_loss: 0.4884 - val_acc: 0.7937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb845ebe48>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Network architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 100, input_length=50))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## Fit the model\n",
    "model.fit(data, np.array(labels), validation_split=0.4, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "futur look like visit v a s latest exhibit futur start here set explor design shape world tomorrow tell you : sinist that s what show pack 112 object product project includ solar - power shirt charg smartphon driverless car which reckon stapl crystal - ball - gaze exhibit year even though engin yet guarante safeti\n",
      "[[ 9429  1606  1811   438 12089   185   556  8865  1595  2717   826  5173\n",
      "   2398   128    24   104  2919   416  2112  3670]]\n",
      "[[0.7231244]]\n"
     ]
    }
   ],
   "source": [
    "test_comment = 'This is my favourite exhibition of all time!! I love the way that the curator placed all the paintings in the correct spots in the room'\n",
    "test_comment2 = \"not a Michelin starred restaurant, but dear me, I also think that you can expect some sort of standard when you go out to eat. Unfortunately, the standard at this restaurant was very poor. Where do I start? Having entered the restaurant, we had to wait for several minutes to be greeted and seated, when finally a blonde waitress said hi to us, or should i say to my husband only, in a very unproffesional flirty manner. She took us to our table, which was a very small square thing, with not much space for anything, and only about 20 cm away from another table, so actually we might as well join the tables together if we go next time (i don't think so !). The menu cards very ABSOLUTELY disgusting, covered in oil and all sorts of dirst with the foil peeling away from the paper . We ordered our drinks, and soon enough the 'over enthusiastic' waitress came back to take our orders , nearly siting down on my husbands lap AND I AM NOT JOKING ! The bruschetta I had as my starter, was nothing more than two slices of burnt Tesco's value garlic bread with a bit of tomato on top, which cost me nearly a fiver ...\"\n",
    "test_comment3 = \"What will the future look like? After visiting the V&A’s latest exhibition, The Future Starts Here, which sets out to explore how design is “shaping the world of tomorrow”, I can tell you: sinister, that’s what. The show packs in 112 objects, products and “projects”, including solar-powered shirts that can charge a smartphone, and driverless cars – which, by my reckoning, have been a staple of crystal-ball-gazing exhibitions for years, even though engineers have yet to guarantee their safety.\"\n",
    "test_comment4 = \"A few years ago, at the J Paul Getty Museum that perches like a city state above a Los Angeles freeway, I asked for directions to the ticket office. There was no admission fee, explained a staffer. “The museum,” she added reverentially, “is Mr Getty’s gift to the world.” This gift was from the same oil squillionaire who, at his sprawling manor in Surrey, installed a guest payphone. Complicated guy. The whole family is quite complicated. I spent much of The Gettys: The World's Richest Arts Dynasty (BBC Two) trying to work out who was descended from whom. But then so did the Gettys. A Getty son once visited the office of his old man, who casually introduced him to a younger brother he’d never met. This documentary was only in passing about complex family dynamics. The main event was the Gettys’ cultural hegemony, and it felt like being hit over the head by a lazy press release. The family, blurbed the voiceover, “used the money to change the art world and British culture forever”. The acquisition of fading film stock “would change the course of British film history”. Getty was described as “the legendary”. So was William Randolph Hearst.\"\n",
    "\n",
    "#3 is from https://www.telegraph.co.uk/tv/2018/04/14/gettys-worlds-richest-arts-dynasty-review-like-hit-head-lazy/\n",
    "#4 is from https://www.telegraph.co.uk/art/reviews/future-starts-va-review-sinister-sobering-picture-tomorrows/\n",
    "\n",
    "cleaned_comment = clean_text(test_comment)\n",
    "comment_to_seq = tokenizer.texts_to_sequences([cleaned_comment])\n",
    "\n",
    "cleaned_comment_2 = clean_text(test_comment2)\n",
    "comment_to_seq_2 = tokenizer.texts_to_sequences([cleaned_comment_2])\n",
    "\n",
    "cleaned_comment_3 = clean_text(test_comment3)\n",
    "print(cleaned_comment_3)\n",
    "comment_to_seq_3 = tokenizer.texts_to_sequences([cleaned_comment_3])\n",
    "\n",
    "input_try = pad_sequences(comment_to_seq_3, maxlen=20)\n",
    "print(input_try)\n",
    "preds = model.predict(input_try)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
